---
title: "Standard Error Estimation"
author: "AECN 396/896-002"
output:
  xaringan::moon_reader:
    # css: [default, metropolis, metropolis-fonts] 
    css: xaringan-themer.css 
    lib_dir: libs
    nature:
      ratio: 4:3
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, child = './../setup.Rmd'}
```

```{r echo = F, eval = F}
setwd("/Users/tmieno2/Dropbox/TeachingUNL/AppliedEconometrics_MS/LectureNotes/Heteroskedasticity/")
```

```{r libraries, include = F, cache = F}
#--- load packages ---#
suppressMessages(library(broom))
suppressMessages(library(fixest))
suppressMessages(library(lmtest))
```

# Before we start 

## Learning objectives

Understand the consequences of the violation of the homoskedasticity assumption and how to deal with the problem

## Table of contents

1. [Review on statistical hypothesis testing](#review)
2. [Testing (linear model)](#linear)
3. [Confidence interval](#conf)

---

class: inverse, center, middle
name: review

# Heteroskedasticity

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle

# Homoskedasticity and Heteroskedasticity  


.pull-left[
.content-box-red[**Homoskedasticity**]

$$Var(u|x) = \sigma^2$$

<br>
.content-box-red[**Heteroskedasticity**]

$$Var(u|x) = f(x)$$

]

---

# Visualization 

```{r echo = F}
set.seed(10384)
N <- 1000

#--- generate data ---#
x <- runif(N,0,1) # x
het_u <- 3*rnorm(N,mean=0,sd=x) # heteroskedastic error
y_het <- 1 + 3*x + het_u # y
hom_u <- 3*rnorm(N,mean=0,sd=0.5) # homoskedastic error
y_hom <- 1 + 3*x + hom_u # y
data <- data.table(
  'Heteroskedastic'=y_het,
  'Homoskedastic'=y_hom,
  x=x) %>%
melt(id.var='x')
data[,type:=factor(variable,levels=c('Homoskedastic','Heteroskedastic'))]

#--- figure ---#
g_het <- ggplot(data=data) +
  geom_point(aes(y=value,x=x),size=0.6) +
  geom_smooth(aes(y=value,x=x),method='lm',se=FALSE) +
  facet_grid(.~type) +
  ylab("") +
  xlab("") +
  theme(
    strip.text = element_text(size = 16)
  )
```

```{r echo = F, out.width = "90%", fig.dim = c(6, 4)}
g_het 
```

---

# Central Questions 


What are the consequences of assuming error is homoskedastic when it is heteroskedastic in reality?

+ Estimation of coefficients $(\hat{\beta}_j)$?
+ Estimation of the variance of $\hat{\beta}_j$?

---

# Coefficient estimators

.content-box-green[**Question**]

--

Are OLS estimators unbiased when error is heteroskedastic?

--

.content-box-green[**Answer**]

--

Yes

--

.content-box-green[**Why?**]

--

We do not homoskedasticity for the OLS estimators to be unbiased.    

---

# Variance of the coefficient estimators 

We learned that when the homoskedasticity assumption holds, then,

$Var(\hat{\beta}_j) = \frac{\sigma^2}{SST_x(1-R^2_j)}$

--

We used the following as the estimator of $Var(\hat{\beta}_j)$

$\frac{\hat{\sigma}^2}{SST_x(1-R^2_j)}$ where $\hat{\sigma}^2 = \frac{\sum_{i=1}^{N} \hat{u}_i^2}{N-k-1}$ 

--

<br>

.content-box-red[**Important**]: By default, R and other statistical software uses this formula to get estimates of the variance of $\hat{\beta}_j$.

.content-box-green[**Note**]: Hereafter, we let $\widehat{Var(\hat{\beta_j})}$ denote the estimator of the variance of $\hat{\beta}_j$.

---

# Variance of the coefficient estimators  

But, under heteroskedasticity,

$Var(\hat{\beta}_j) \ne \frac{\sigma^2}{SST_x(1-R^2_j)}$

--

.content-box-green[**Question**]:
   
Is $E[\widehat{Var(\hat{\beta}_j)}]=E\Big[\frac{\hat{\sigma}^2}{SST_x(1-R^2_j)}\Big]=Var(\hat{\beta}_j)$ under heteroskedasticity?

--

.content-box-green[**Answer**]: No

---

# Variance of the coefficient estimators  

.content-box-green[**Question**]:

So, what are the consequences of using $\widehat{Var(\hat{\beta}_j)}=\frac{\hat{\sigma}^2}{SST_x(1-R^2_j)}$ under heteroskedasticity?


.content-box-red[**Consequence**]

Your hypothesis testing is going to be biased.

---

# Consequence of heteroskedasticity on testing

Let's run MC simulations to see the consequence of ignoring heteroskedasticity.

--

.content-box-green[**Model**]

$y = 1 + \beta x + u$, where $\beta = 0$

--

.content-box-red[**Test of interest**]

+ $H_0:$ $\beta=0$
+ $H_1:$ $\beta \ne 0$

--

.content-box-green[**Question**]

If you test the null hypothesis at the $5\%$ significance level, what should be the probability that you reject the null hypothesis when it is actually true? 

$Pr(\mbox{reject} \;\; H_0|H_0 \;\; \mbox{is true})=?$

---

.content-box-green[**Question**]

If you test the null hypothesis at the $5\%$ significance level, what should be the probability that you reject the null hypothesis when it is actually true? 

```{r echo = F, out.width = "40%"}
u <- seq(-3.5, 3.5, length = 1000)
df <- 500 - 10
pdf <- dt(u, df = df) # default is mean = 0,sd = 1
data <- data.table(x = u, y = pdf)

alpha <- 0.05

g_t <- ggplot() +
  geom_line(data=data,aes(y=y,x=x)) +
  geom_hline(yintercept=0) +
  xlab('') +
  ylab('') +
  theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()
    )

c_rej <- qt(1 - alpha/2, df = df)
rej_data_high <- data[u >= c_rej,]
rej_data_low <- data[u <= -c_rej,]

g_t +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = 'red', alpha = 0.4
    ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y,ymin = 0,x = x),
    fill = 'red',alpha = 0.4
    ) +
  annotate('text',x = c_rej,y = -0.02,
    label = as.character(round(c_rej,digits = 2)),
    size = 4,
    family = 'Times'
    ) +
  annotate('text',x = 0, y = 0.1,
    label = paste('area  = ',round(alpha/2,digits = 3),sep = ''),
    size = 4,
    family = 'Times'
    ) 
```

--

.content-box-green[**Answer**]

Since the null is true (we generate the data that way!), the probability you reject the null should be the same as the significance level, which is $5%$.

---

# MC simulation: conceptual steps

+ generate a dataset so that $\beta_1$ (the coefficient on $x$) is zero

$$y=\beta_0+\beta_1 x + v$$

+ estimate the model and find $\hat{\beta}_1$ and $\widehat{se(\hat{\beta}_1)}$
+ calculate $t$-statistic $(\hat{\beta}_x-0/\widehat{se(\hat{\beta}_x)})$ and decide whether you reject the null or not
+ repeat the above 1000 times
+ check how often you reject the null (should be close to 50 times)

---

# MC simulation: R code 

```{r }
set.seed(927834)

N <- 1000 # number of observations
B <- 1000 # number of simulations

b_hat_store <- rep(0, B) # beta hat storage
t_stat_store <- rep(0, B) # t-stat storage
c_value <- qt(0.975, N - 2) # critical value 

x <- runif(N,0,1) # x (fixed across iterations)

for (i in 1:B){
  #--- generate data ---#
  het_u <- 3 * rnorm(N, mean = 0, sd = 2 * x) # heteroskedastic error
  y <- 1 + het_u # y
  data_temp <- data.frame(y = y,x = x)

  #--- regression ---#
  ols_res <- lm(y~x,data=data_temp)  

  b_hat <- ols_res$coef['x'] # coef estimate on x
  b_hat_store[i] <- b_hat # save the coef estimate
  vcov_ols <- vcov(ols_res) # get variance covariance matrix
  t_stat_store[i] <- b_hat/sqrt(vcov_ols['x','x']) # calculate t-stat
} 
```

---

# MC simulation: Results

```{r }
#--- how many times do you reject? ---#
reject_or_not <- abs(t_stat_store) > c_value
  mean(reject_or_not) 
```

--

.content-box-red[**Consequence of ignoring heteroskedasticity**]

We rejected the null hypothesis `r mean(reject_or_not)*100`% of the time, instead of $5\%$.

--

+ So, you are more likely to claim that $x$ has a statistically significant impact than you are supposed to.

--

+ The use of the formula $\frac{\hat{\sigma}^2}{SST_x(1-R^2_j)}$ seemed to (over/under)-estimate the true variance of the OLS estimators

--

+ The direction of bias is ambiguous. 

---

# How should we address this problem?

+ Now, we understand the consequence of heteroskedasticity: 

$\frac{\hat{\sigma}^2}{SST_x(1-R^2_j)}$ is a biased estimator of $Var(\hat{\beta})$, which makes any kind of testings based on it invalid.

--

+ Can we credibly estimate the variance of the OLS estimators?

--

<br>

.content-box-red[**White-Huber-Eicker heteroskedasticity-robust standard error estimator**]

--

+ valid in the presence of heteroskedasticity of .red[unknown form]
+ heteroskedasticity-robust standard error estimator in short

---
class: middle

.content-box-red[**Heteroskedasticity-robust standard error estimator**]

$\widehat{Var(\hat{\beta}_j)} = \frac{\sum_{i=1}^n \hat{r}^2_{i,j} \hat{u}^2_i}{SSR^2_j}$

+ $\hat{u}_i$: residual from regressing $y$ on all the independent variables
+ $\hat{r}_{i,j}$: residual from regressing $x_j$ on all other independent variables for $i$th observation
+ $SSR^2_j$: the sum of squared residuals from regressing $x_j$ on all other independent variables

---
class: middle

We spend .red[NO] time to try to understand what's going on with the estimator.

What you need is

+ understand the consequence of heteroskedasticity
+ know there is an estimator that is appropriate under heteroskedasticity (heteroskedasticity-robust standard error estimator)
+ know how to use the heteroskedasticity-robust standard error estimator in $R$ (or some other software)

---

# In practice 

Here is the well-accepted procedure in econometric analysis:

--

+ Estimate the model using OLS (you do nothing special here)

--

+ Assume the error term is heteroskedastic and estimate the variance of the OLS estimators 
  * There are tests to whether error is heteroskedastic or not: .red[Breusch-Pagan] test and .red[White] test
  * In practice, almost nobody bothers to conduct these tests
  * We do not learn how to run these tests

--

+ Replace the estimates from $\widehat{Var(\hat{\beta})}_{default}$ with those from $\widehat{Var(\hat{\beta})}_{robust}$ for testing

--

+ But, we do not replace coefficient estimates (remember, coefficient estimation is still unbiased under heteroskedasticity)

---
 
# Implementation in R 

We use

+ the `vcovHC()` function from the `sandwich` package to estimate heteroskedasticity-robust standard errors

+ the `coeftest()` function from the `lmtest` package to do tests

--

Let's run a regression using `MLB1.dta`.

```{r }
#--- library ---#
library(readstata13)

#--- import the data ---#
mlb_data <- read.dta13('MLB1.dta')

#--- regression ---#
reg_mlb <- lm(log(salary)~years+bavg,data=mlb_data) 
``` 

---
class: middle

.content-box-red[**Obtaining Heteroskedasticity-robust SE estimates**]

```{r }
library(sandwich)
```

```{r eval = F}
#--- NOT RUN ---#  
vcovHC(regression result)
```

---


.content-box-red[**Default**]

```{r }
vcov_hom <- vcov(reg_mlb)
```

```{r echo = F}
vcov_hom 
```

<br>

.content-box-red[**Heteroskedasticity-robust**]

```{r }
vcov_het <- vcovHC(reg_mlb)
```

```{r echo = F}
vcov_het 
``` 

---
class: middle

.content-box-red[**Coefficient tests using `coeftest()`**]

```{r eval = F}
library(lmtest)
```

```{r eval = F}
#--- NOT RUN ---#  
coeftest(regression result, variance covariance matrix)
```

---

.content-box-red[**Default**]

```{r }
coeftest(reg_mlb, vcov_hom) %>% tidy()
```

<br>

.content-box-red[**Heteroskedasticity-robust**]

```{r }
coeftest(reg_mlb, vcov_het) %>% tidy()
```

---

# Het-robust SE estimator: validation 

Does the heteroskedasticity-robust se estimator work? Let's see using MC simulations:

```{r }
#--- x fixed across iterations ---#
x <- runif(N,0,1) # x

for (i in 1:B){
  #--- generate data ---#
  het_u <- 3 * rnorm(N, mean = 0, sd = 2 * x) # heteroskedastic error
  y <- 1 + het_u # y
  data_temp <- data.frame(y = y, x = x)

  #--- regression ---#
  ols_res <- lm(y~x,data = data_temp)
  b_hat <- ols_res$coef['x'] # coef estimate on x
  b_hat_store[i] <- b_hat # save the coef estimate
  vcov_hc <- vcovHC(ols_res) # get variance covariance matrix #<<
  t_stat_store[i] <- b_hat/sqrt(vcov_hc['x','x']) # calculate t-stat
}  
```

---

# MC simulation results 

```{r }
reject_or_not <- abs(t_stat_store) > c_value
mean(reject_or_not)
```

Okay, not perfect. But, certainly better. 

---

class: inverse, center, middle
name: review

# Clustered Error

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

---
class: middle


.content-box-red[**Clustered Error**]

+ Often times, observations can be grouped into clusters
+ Errors within the cluster can be correlated

---
class: middle

.content-box-red[**Example 1**]

College GPA: cluster by college

$GPA_{col} = \beta_0 + \beta_1 income + \beta_2 GPA_{hs} + u$

+ Your observations consist of students' GPA scores across many colleges
+ Because of some unobserved (omitted) school characteristics, error terms for the individuals in the same college might be correlated.
  * grading policy

---
class: middle

.content-box-red[**Example 2**]

Eduction Impacts on Income: cluster by individual

+ Your observations consist of 500 individuals with each individual tracked over 10 years

+ Because of some unobserved (omitted) individual characteristics, error terms for time-series observations within an individual might be correlated.
  * innate ability

---

# Consequences of clustered error 

.content-box-green[**Question**]

Are the OLS estimators of the coefficients biased in the presence of clustered error?

--

.content-box-green[**Answer**]

--

No, the correlation between $x$ and $u$ would hurt you, but not correlation among $u$.

---

# Consequences of clustered error 

.content-box-green[**Question**]

Are $\widehat{Var(\hat{\beta})}_{default}$ unbiased estimators of $Var(\hat{\beta})$?

--

.content-box-green[**Answer**]

--

No, $\widehat{Var(\hat{\beta})}_{default}$ is unbiased only under homoskedasticity assumption, which assumes no correlation between errors

---

# Consequences of clustered error 

.content-box-green[**Question**]

Which has more information?
+ two errors that are independent
+ two errors that are correlated

<br>
.content-box-red[**Consequences**]

+ If you were to use $\widehat{Var(\hat{\beta})}_{default}$ to estimate $Var(\hat{\beta})$ in the presence of clustered error, you would (under/over)-estimate the true $Var(\hat{\beta})$.

+ This would lead to rejecting null hypothesis (more/less) often than you are supposed to

---

# MC simulations: conceptual steps

Here are the conceptual steps of the MC simulations to see the consequence of clustered error. 

+ generate data according to the generating process in a way that error within the cluster (two clusters in this example) is correlated
+ estimate the model and find $\hat{\beta}_x$ and $\widehat{se(\hat{\beta}_x)}$
+ calculate $t$-statistic $(\hat{\beta}_x/\widehat{se(\hat{\beta}_x)})$
+ repeat steps 1-3 for 1000 times
+ see how many times out of 1000 times you reject the null hypothesis: $H_0:$ $\beta_x=0$

---

# R code: steps 1 - 3

Here is the R code for one iteration (Steps 1 ~ 3):

```{r }
#--- setup ---#
library(MASS) # to use the mvrnorm() function later
N <- 200 # number of observations per cluster

#--- error term ---#
# generating errors that are correlated within each group
u_1 <- mvrnorm(N,mu=rep(0,N),Sigma=matrix(.7,nrow=N,ncol=N)+diag(N)*.3)
u_2 <- mvrnorm(N,mu=rep(0,N),Sigma=matrix(.7,nrow=N,ncol=N)+diag(N)*.3)

#--- x ---#
x_1 <- runif(N); x_2 <- runif(N) 

#--- y ---#
y_1 <- 1 + 0*x_1 + u_1; y_2 <- 1 + 0*x_2 + u_2

#--- data ---#
data <- data.table(y=c(y_1,y_2),x=c(x_1,x_2)) 

#--- OLS ---#
reg <- lm(y~x,data=data)
vcov_temp <- vcov(reg) # default #<<
t_stat <- reg$coef['x']/sqrt(vcov_temp['x','x'])
t_stat_store[i] <- t_stat
```

---

# R code: step 4

```{r cluster-MC}
B <- 1000
t_stat_store <- rep(0,B)
for (i in 1:B){
  #--- generate data ---#
  u_1 <- mvrnorm(N,mu=rep(0,N),Sigma=matrix(.7,nrow=N,ncol=N)+diag(N)*.3)
  u_2 <- mvrnorm(N,mu=rep(0,N),Sigma=matrix(.7,nrow=N,ncol=N)+diag(N)*.3)
  x_1 <- runif(N) # x for group 1
  x_2 <- runif(N) # x for group 2
  y_1 <- 1 + 0*x_1 + u_1
  y_2 <- 1 + 0*x_2 + u_2
  data <- data.table(y=c(y_1,y_2),x=c(x_1,x_2))

  #--- OLS ---#
  reg <- lm(y~x,data=data)

  #--- get vcov ---#
  vcov_temp <- vcov(reg)

  #--- calculate t-stat ---#
  t_stat <- reg$coef['x']/sqrt(vcov_temp['x','x'])
  t_stat_store[i] <- t_stat
} 
```

---

# MC simulations: results 

```{r , dependson = "cluster-MC"}
c_value <- qt(0.975,400-2)

#--- how often do you reject the null ---#
mean(abs(t_stat_store) > c_value) 
```

--


.content-box-red[**Important**]: 

+ clustered error can severely bias your test results
+ it is always to make variables more significant than they truly are

---
  
# What to do? 


.content-box-red[**Cluster-robust standard error estimation**]

There exist estimators of $Var(\hat{\beta})$ that take into account the possibility that errors are clustered.

+ We call such estimators .blue[cluster-robust variance covariance estimator] denoted as $(\widehat{Var(\hat{\beta})}_{cl})$

+ We call estimates from such estimators .blue[cluster-robust variance]

---

# Cluster-robust standard error estimation 

I neither derive nor show the mathematical expressions of these estimators.
  

.content-box-red[**This is what you need to do**]

+ understand the consequence of clustered errors

+ know there are estimators that are appropriate under clustered error

+ know that the estimators we will learn take care of heteroskedasticity at the same time (so, they really are cluster- and heteroskedasticity-robust standard error estimators)

+ know how to use the estimators in $R$ (or some other software)

---

# R implementation 

We use the `feols()` package from the `fixest` package. 

This is the function we will be using instead of `lm()` for the rest of the course.

<br>

.content-box-red[**feols()**]

```{r eval = F}
feols(dep var ~ indep var 1 + indep var 2 + ..., data = data) 
```

So, syntactically, it is identical with `lm()`, but it goes way beyond `lm()` as we will see later when we cover panel data estimation methods.

--

<br>

.content-box-red[**Cluster-robust standard error**]

Run a regression, and then apply `vcov()` with the `cluster = ~ ` option.

---

# Before an R demonstration 

Let's take a look at the MLB data again.

```{r }
mlb_data[, c("salary", "years", "bavg", "nl")] %>% head()
```

`nl` (1 if in the National league, 0 if in the American league) is the group variable we cluster around. 

---

# R Demonstration 


.content-box-red[**Step 1**]

Run a regression

```{r }
reg_mlb <- feols(log(salary) ~ years + bavg, data = mlb_data) 
```

--

.content-box-red[**Step 2**]

Apply `vcov()` with the `cluster =` option.


```{r eval = F}
vcov(reg_mlb, cluster = ~ nl)
```

```{r echo = F}
vcov_cl <- vcov(reg_mlb, cluster = ~ nl)
attr(vcov_cl, "type") <- NULL
attr(vcov_cl, "dof.type") <- NULL
attr(vcov_cl, "dof.K") <- NULL
vcov_cl
```

Here, it is clustered around `nl`.

---

# Compare 


.content-box-red[**Default**]

```{r eval = F}
vcov(reg_mlb) 
```

```{r echo = F}
vcov_default <- vcov(reg_mlb)
attr(vcov_default, "type") <- NULL
attr(vcov_default, "dof.type") <- NULL
attr(vcov_default, "dof.K") <- NULL
vcov_default 
```

.content-box-red[**Cluster-robust standard error**]

```{r eval = F}
vcov(reg_mlb, cluster = ~ nl) 
```

```{r echo = F}
vcov_cl
```
---

# In practice 

Just like the heteroskedasticity-present case before,

+ Estimate the model using OLS (you do nothing special here)

+ Assume the error term is clustered and/or heteroskedastic, and estimate the variance of the OLS estimators ($Var(\hat{\beta})$) using cluster-robust standard error estimators

+ Replace the estimates from $\widehat{Var(\hat{\beta})}_{default}$ with those from $\widehat{Var(\hat{\beta})}_{cl}$ for testing

+ But, we do not replace coefficient estimates.
       
---

# But does it really work? 

Let's run MC simulations to see if the use of the cluster-robust standard error estimation method works:

--

Here is the data generating process:

```{r }
#--- error correlated within group ---#
u_1 <- mvrnorm(N, mu=rep(0, N), Sigma=matrix(.7, nrow=N, ncol=N)+diag(N)*.3)
u_2 <- mvrnorm(N, mu=rep(0, N), Sigma=matrix(.7, nrow=N, ncol=N)+diag(N)*.3)

#--- other variables ---#
x_1 <- runif(N) # x for group 1
x_2 <- runif(N) # x for group 2
y_1 <- 1 + 1*x_1 + u_1 # dep var for group 1
y_2 <- 1 + 1*x_2 + u_2 # dep var for group 2
g_1 <- rep('group 1', N) # group 1 #<<
g_2 <- rep('group 2', N) # group 2 #<<

#--- put together all the variables ---#
data <- data.frame(y=c(y_1, y_2), x=c(x_1, x_2), group=c(g_1, g_2))  

```

---
class: middle

Take a look at the data:

```{r }
data %>% head()
```

We are going to use `group` to cluster standard error estimation.

---

# MC simulation results: R code 

```{r cl-MC}
B <- 1000
t_stat_store <- rep(0, B)
for (i in 1:B){
  #--- generate data ---#
  u_1 <- mvrnorm(N, mu = rep(0, N), Sigma = matrix(.7, nrow = N, ncol = N)+diag(N)*.3)
  u_2 <- mvrnorm(N, mu = rep(0, N), Sigma = matrix(.7, nrow = N, ncol = N)+diag(N)*.3)
  x_1 <- runif(N) # x for group 1
  x_2 <- runif(N) # x for group 2
  y_1 <- 1 + 0*x_1 + u_1
  y_2 <- 1 + 0*x_2 + u_2
  data <- data.table(y = c(y_1, y_2), x = c(x_1, x_2), group = c(rep(1, N), rep(2, N)))

  #--- OLS ---#
  reg <- feols(y ~ x,  data = data)

  #--- get cluster-robust vcov ---#
  vcov_cl <- vcov(reg, cluster = ~ group)

  #--- calculate t-stat ---#
  t_stat <- reg$coefficients['x']/sqrt(vcov_cl['x', 'x'])
  t_stat_store[i] <- t_stat
}  
```

---

# MC simulation results 

```{r }
#--- critical value ---#
c_value <- qt(0.975, 400-2)

#--- how often do you reject the null ---#
mean(abs(t_stat_store) > c_value) 
```

--

Well, it is much better than the default VCOV, but there is still a long way to go. 

--

.content-box-red[**Important**]

+ Cluster-robust standard error estimation gets better as the number of groups get larger 

+ The number of groups of 2 is too small (As a rule of thumb, \# of groups larger than 50 is sufficiently large)





\documentclass[fleqn]{beamer}

% \usefonttheme[onlylarge]{structuresmallcapsserif}
\usefonttheme[onlysmall]{structurebold}
\usepackage[normalem]{ulem} % use normalem to protect \emph
\newcommand\hl{\bgroup\markoverwith
  {\textcolor{yellow}{\rule[-.5ex]{2pt}{2.5ex}}}\ULon}
\usepackage{amsmath,amssymb}
\usepackage{tikz-cd}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[all]{xy}
\usepackage{lscape}
\usepackage[authoryear]{natbib}
\usepackage{array}
\usepackage{listings}
\usepackage{wasysym}
\usepackage{url}
\usepackage{multirow}
\usepackage{color}
\usepackage{qtree}
\usepackage[absolute,overlay]{textpos}
\usepackage{inconsolata}
\usepackage{framed}
\definecolor{shadecolor}{rgb}{1,0.8,0.3}
\usepackage[justification=centering]{caption}
\captionsetup{compatibility=false}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue}
\usepackage[space]{grffile}
\usefonttheme[onlymath]{serif}
% \usepackage{tikz}
% \usetikzlibrary{calc,matrix}
\usepackage{sgame}
\usepackage{tikz}
\usetikzlibrary{trees}

\mode<presentation>
%\usetheme{Frankfurt}
\usetheme{boxes}
\usecolortheme{beaver}
%\usetheme{Singapore}
%\usetheme{Hannover}

\newcommand*{\captionsource}[2]{%
  \caption[{#1}]{%
    #1%
    \\\hspace{\linewidth}%
    {\small\textbf{Source:} #2}%
  }
}

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{blocks}[rounded]
\newenvironment<>{block_code}[1]{%
  \setbeamercolor{block title}{fg=white,bg=black}%
  \begin{block}#2{#1}}{\end{block}}

%--------------------------
% Reduce the spacing between R code and output
%--------------------------
\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
\makeatother

\renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}

%===================================
% Coloring change
%===================================
\definecolor{darkred}{rgb}{0.8,0,0}
\setbeamercolor{block title}{fg=darkred,bg=structure.fg!20!bg!50!bg}
\setbeamercolor{block body}{use=block title,bg=block title.bg}


%===================================
% knitr preparation
%===================================

<<prep, echo=FALSE, message=F, warning=F>>=
source('~/Box/R_libraries_load/library.R')
library(stargazer)
library(readstata13)
source('~/Box/MySoftware/MyR/ggplot2/ggplot2_default_teaching.R')
source('~/Box/Teaching/R_functions/functions.R')
opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  tidy=FALSE,
  size='footnotesize',
  #--- figure related ---#
  fig.align='center',
  out.height='2.5in',
  out.width='2.5in',
  dev='pdf'
  )
# setwd('~/Box/Teaching/UNL/EconometricsMaster/aecn892_2019/lectures/DiscreteChoice')
@

%===================================
% Title
%===================================
\title{Discrete Choice Analysis}
\author{Taro Mieno}
\date{AECN 896-003: Applied Econometrics}
\everymath{\displaystyle}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%===================================
% Functional Form
%===================================

\begin{frame}[c,fragile]
  \begin{block}{Discrete Choice Analysis}
    \begin{itemize}
      \item Focus on understanding choices that are discrete (not continuous)
      \begin{itemize}
        \item Whether you own a car or not (binary choice)
        \item Whether you use an iPhone, Android, or other types of cell phones (Multinomial choice)
        \item Which recreation sites you visit this winter (multinomial)
      \end{itemize}
      \item Linear models we have seen are often not appropriate
    \end{itemize}
  \end{block}
\end{frame}

\title{Binary Response Model}
\author{}
\date{}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}[c]
  \begin{block}{Binary Response}
  \vspace{-0.6cm}
    \begin{align*}
    &y = 0 \;\; \mbox{(if you do not own a car)} \;\; \\
    &y = 1 \;\; \mbox{(if you own at least one car)} \;\;
    \end{align*}
  \end{block}
  \begin{block}{Question we would like to answer}
    How do independent variables $x_1,\dots,x_k$ affect the status of $y$ (the choice of whether to own at least one car or not)?
  \end{block}
\end{frame}

% \begin{frame}[c]
%   \begin{block}{Continuous dependent variable}
%   \vspace{-0.6cm}
%     \begin{align*}
%      \mbox{In general:}\;\; & E[y|x_1,\dots,x_k] = \beta_0+\beta_1 x_1+\dots+\beta_k x_k \\
%      \mbox{An example:}\;\; & E[income|educ,exper] = \beta_0+\beta_1 educ+\beta_2 exper
%   \end{align*}
%   \end{block}
%
%   \begin{block}{Binary choice}
%     We are interested in exactly the same thing:
%   \begin{align*}
%   E[y|x_1,\dots,x_k], \;\; \mbox{which is actually}\;\; Prob(y=1|x_1,\dots,x_k)
%   \end{align*}
%   \vspace{-0.6cm}
%   \textcolor{blue}{
%   \begin{align*}
%   (E[y|x_1,\dots,x_k]=&Prob(y=0|x_1,\dots,x_k)\times 0\\
%                       &+Prob(y=1|x_1,\dots,x_k)\times 1)
%   \end{align*}
%   }
%   \end{block}
% \end{frame}

\begin{frame}[c]
   \begin{block}{Binary Response Model}
  We try to model the \textcolor{blue}{probability} of $y=1$ (own at least one car)
    \begin{align*}
    Pr(y=1|x_1,\dots,x_k) = f(x_1,\dots,x_k)
    \end{align*}
  as a function of independent variables.
  \end{block}
  \only<1-1>{\begin{block}{Linear probability model}
  \begin{align*}
    Pr(y=1|x_1,\dots,x_k) = \beta_0+\beta_1 x_1+\dots+\beta_k x_k
  \end{align*}
  \vspace{-0.6cm}
  \only<2->{\begin{itemize}
    \item \textcolor{blue}{probability is not bounded within $[0,1]$}
  \end{itemize}}
  \end{block}}

  \only<2->{\begin{block}{So, how about}
  \begin{align*}
    Pr(y=1|x_1,\dots,x_k) = G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)
  \end{align*}
  such that $G()$ is a function taking on values strictly between zero and once: $0<G(z)<1$ for all real numbers $z$?
  \end{block}}

\end{frame}

\begin{frame}[c]
  \begin{block}{Notes}
     Different choices of $G()$ lead to different models
   \end{block}
   \begin{block}{Logit model}
   \vspace{-0.6cm}
   \begin{align*}
    G(z) = exp(z)/[1+exp(z)] = \frac{e^z}{1+e^z}
   \end{align*}
   \end{block}

   \begin{block}{Probit model}
   \vspace{-0.6cm}
   \begin{align*}
    G(z) = \Phi(z)
   \end{align*}
   where $\Phi(z)$ is the standard normal cumulative distribution function
   \end{block}
\end{frame}

\begin{frame}[c,fragile]
<<pro_log, echo=FALSE, cache=TRUE, size='scriptsize'>>=
  plot_data <- data.table(z = seq(-3,3,length=1000)) %>%
    mutate(logit = exp(z)/(1+exp(z)), normal = pnorm(z)) %>%
    melt(id.vars='z') %>%
    data.table()

  g_G <- ggplot(data=plot_data) +
    geom_line(aes(x=z,y=value,color=variable),size=1) +
    ylab('y') +
    scale_color_discrete(name='') +
    theme(
      legend.position='bottom'
    )
  g_G
@
\end{frame}

\begin{frame}[c]
  \begin{block}{Interpretation}
  \vspace{-0.4cm}
    \begin{align*}
    Pr(y=1|x_1,\dots,x_k) = G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)
    \end{align*}
  \vspace{-0.6cm}
    \begin{itemize}
      \item What do $\beta$s measure?
      \item How do we interpret them?
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \only<1-1>{\begin{block}{Before and after}
  \vspace{-0.6cm}
  \begin{align*}
    \mbox{Before:}\;\;& x_1=0,\dots,x_k=0 \Rightarrow z=\beta_0 \\
    \mbox{After:}\;\;& x_1=1 \;\;\mbox{and}\;\; x_2=0,\dots,x_k=0 \Rightarrow z=\beta_0+\beta_1 x_1
  \end{align*}
  \end{block}}

<<inter, echo=FALSE, cache=TRUE, size='scriptsize'>>=
  ggplot(data=plot_data[variable=='normal',]) +
    geom_line(aes(x=z,y=value),size=1) +
    geom_hline(yintercept=0,size=0.2) +

    #--- before ---#
    geom_point(aes(y=0,x=-1),color='red') +
    geom_text(aes(y=-0.04,x=-1,label='before'),color='blue',size=6) +
    geom_point(aes(y=pnorm(-1),x=-1),color='red') +
    geom_segment(aes(y=pnorm(-1),x=-1,yend=0,xend=-1),color='red',linetype=2) +

    #--- after ---#
    geom_point(aes(y=0,x=1),color='red') +
    geom_text(aes(y=-0.04,x=1,label='after'),color='blue',size=6) +
    geom_point(aes(y=pnorm(1),x=1),color='red') +
    geom_segment(aes(y=pnorm(1),x=1,yend=0,xend=1),color='red',linetype=2) +

    #--- change in z ---#
    geom_segment(
      aes(x=-1,y=0,xend=1,yend=0),
      arrow = arrow(length = unit(0.05, "npc")),
      color='red',size=0.5
      ) +

    ylab('y') +
    scale_color_discrete(name='') +
    theme(
      legend.position='bottom'
    )
@
  \only<2->{
  \begin{itemize}
    \item $\beta$s measure how far you move along the x-axis
    \item $\beta$s does not directly measure how independent variables influence the probility of $y=1$
  \end{itemize}

  }
\end{frame}

\begin{frame}[c]
  \begin{block}{Marginal impact of $x_k$ (continuous) on $Prob(y=1)$}
    \begin{align*}
    Pr(y=1|x_1,\dots,x_k) = G(z)  \\
    z = \beta_0+\beta_1 x_1+\dots+\beta_k x_k
    \end{align*}
    Differentiating both sides with respect to $x_k$,
    \begin{align*}
    \frac{\partial Pr(y=1|x_1,\dots,x_k)}{\partial x_k} & = G'(z)\times \frac{\partial z}{\partial x_k} \\
    & = G'(z)\times \beta_k
    \end{align*}
  \end{block}
  \only<2->{\begin{block}{Notes}
    \begin{itemize}
      \item The marginal impact of an independent variable depends on the values of all the independent variables: $G(\beta_0+\beta_1 x_1+\dots+\beta_k x_k)$
      \item Since $G'()$ is always positive, the sign of the marginal impact of an independent variable on $Prob(y=1)$ is always the same as the sign of its coefficient
    \end{itemize}
  \end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{Estimation of Binary Choice Models}
    \begin{itemize}
    \item Linear models: OLS
    \item Binary choice models: \textcolor{blue}{Maximum Likelihood Estimation (MLE)}
    \end{itemize}
  \end{block}
  \begin{block}{OLS}
    Find parameters that makes the sum of residuals squared the smallest
  \end{block}
  \begin{block}{MLE (very loosely put)}
    Find parameters ($\beta$s) that makes what we observed (collection of binary decisions made by different individuals) most likely (\textcolor{blue}{Maximum Likelihood})
  \end{block}
\end{frame}

\begin{frame}[c]
  \only<1-3>{\begin{block}{Decisions made by two individuals}
  \begin{itemize}
  \item Individual 1: $y=1$ (own at least one car)
  \item Individual 2: $y=0$ (does not own a car)
  \end{itemize}
  \end{block}}

  \only<2->{
  \begin{block}{Probability of individual decisions}
  \vspace{-0.6cm}
    \begin{align*}
    \mbox{Individual 1}:& Prob(y_1=1|\mathbf{x_1})= G(z_1) \\
    \mbox{Individual 2}:& Prob(y_2=0|\mathbf{x_2})= 1-G(z_2)
    \end{align*}
  \end{block}
  }

  \only<3->{
  \begin{block}{Probability of a collection of decisions}
  The probability that we observe a \textcolor{blue}{collection of choices} made by them (if their decisions are independent)
    \begin{align*}
      Prob(y_1=1|\mathbf{x_1})\times Prob(y_2=0|\mathbf{x_2}) = G(z_1)\times [1-G(z_2)],
    \end{align*}
  which we call \textcolor{blue}{likelihood function}.
  \end{block}
  }

  \only<4->{
  \begin{block}{MLE}
  \vspace{-0.6cm}
    \begin{align*}
      Max_{\beta_1,\dots,\beta_k}\;\; & G(z_1)\times [1-G(z_2)]
    \end{align*}
  \end{block}
  }

\end{frame}

\begin{frame}[c]
  \begin{block}{MLE of Binary Choice Model in General}
  Maximize the likelihood function:
  \begin{align*}
     Max_{\beta_1,\dots,\beta_k}\;\; L
  \end{align*}
  where $L=\Pi_{i=1}^n \Big[y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big]$ is the likelihood function.
  \end{block}
  \begin{block}{Log-likelihood function}
  \vspace{-0.6cm}
  \begin{align*}
    LL & = log\Big(\Pi_{i=1}^n \Big[y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big]\Big) \\
       & = \sum_{i=1}^n log\Big(y_i\times G(z_i)+(1-y_i)\times(1-G(z_i))\Big)
  \end{align*}
  \end{block}
  \begin{block}{MLE with $LL$}
  \vspace{-0.6cm}
  \begin{align*}
    argmax_{\beta_1,\dots,\beta_k}\;\; L \equiv argmax_{\beta_1,\dots,\beta_k}\;\; LL
  \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Implementation in R with an example}
  Participation of females in labor force:
  \begin{align*}
  Pr(inlf=1|\mathbf{x})= G(z)
  \end{align*}
  where
  \begin{align*}
  z & = \beta_0+\beta_1 nwifeinc+ \beta_2 educ+ \beta_3 exper + \\
    & + \beta_4 exper^2 + \beta_5 age + \beta_6 kidslt6 + \beta_7 kidsge6
  \end{align*}
  \begin{itemize}
    \item $inlf$: 1 if in labor force in 1975, 0 otherwise
    \item $nwifeinc$: earning as a family if she does not work
    \item $kidslt6$: \# of kids less than 6 years old
    \item $kidsge6$: \# of kids who are 6-18 year old
  \end{itemize}

  \end{block}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: importing the data}
<<data, cache=TRUE, size='scriptsize'>>=
  #--- import the data ---#
  data <- read.dta13('MROZ.dta') %>%
    mutate(exper2=exper^2)

  #--- take a look ---#
  dplyr::select(data,inlf,nwifeinc,kidslt6,kidsge6,educ) %>%
    head()
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Estimating binary choice model using $R$}
    You can use the \textcolor{blue}{$glm$()} function (no new packages installation necessary) when using cross-sectional data
    \begin{itemize}
      \item $glm$ refers to Generalized Linear Model, which encompass linear models we have been using
      \item you specify the $family$ option to tell what kind of model you are estimating
    \end{itemize}
  \end{block}


\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Probit model estimation using glm()}

<<probit, cache=TRUE, size='scriptsize'>>=
    probit_lf <- glm(

    #--- formula ---#
    inlf~nwifeinc+educ+exper+exper2+age+kidslt6+kidsge6,

    #--- data ---#
    data = data,

    #--- models ---#
    family=binomial(link='probit')
    )
@
  \end{block_code}
  \begin{block}{$family$ option}
    \begin{itemize}
      \item $binomial()$: tells $R$ that your dependent variable is binary
      \item $link='probit'$: tells $R$ that you want to use the cumulative distribution function of the standard normal distribution as $G()$ in $Prob(y=1|\mathbf{x})=G(z)$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Probit model estimation using glm()}

<<probit_sum, cache=TRUE, size='scriptsize'>>=
  summary(probit_lf)$coef
@

  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Logit model estimation using glm()}

<<logit, cache=TRUE, size='scriptsize'>>=
  logit_lf <- glm(

  #--- formula ---#
  inlf~nwifeinc+educ+exper+exper2+age+kidslt6+kidsge6,

  #--- data ---#
  data = data,

  #--- models ---#
  family=binomial(link='logit')
  )

@
  \end{block_code}
  \begin{block}{$family$ option}
    \begin{itemize}
      \item $binomial()$: tells $R$ that your dependent variable is binary
      \item $link='logit'$: tells $R$ that you want to use the function as $G()$ in $Prob(y=1|\mathbf{x})=G(z)$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{Logit model estimation results}

<<logit_sum, cache=TRUE, size='scriptsize'>>=
  summary(logit_lf)$coef
@

  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
<<eval=FALSE, size='scriptsize'>>=
stargazer(probit_lf,logit_lf,type='latex',no.space=TRUE,table.layout='-ldc-t-s-')
@

\scriptsize{
<<stargaze, echo=FALSE, cache=TRUE, size='scriptsize',results='asis'>>=
stargazer(probit_lf,logit_lf,type='latex',no.space=TRUE,table.layout='-ldc-t-s-')
@
}
\end{frame}

\begin{frame}[c,fragile]
  \frametitle{Post-estimation operations and diagnostics}
  \begin{block}{Log-likelihood}
    \begin{align*}
      LL =\sum_{i=1}^n log\Big(y_i\times G(\hat{z}_i)+(1-y_i)\times(1-G(\hat{z}_i))\Big)
    \end{align*}
    \begin{itemize}
      \item $\hat{z}_i=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k$
      \item $G(\hat{z}_i)$ is the fitted value of $Prob(y=1|\mathbf{x})$
    \end{itemize}
    \textcolor{blue}{(The greater (less negative) the LL, the better the fit of the regression)}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{McFadden's pseudo-$R^2$}
    A measure of how much better your model is compared to the model with only the intercept
    \begin{align*}
    pseudo-R^2=1-LL/LL_0
    \end{align*}
    where $LL_0$ is the log-likelihood when you include only the intercept
  \end{block}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: pseudo-$R^2$}
<<r2, cache=TRUE, size='scriptsize'>>=
  #--- estimate the model with only the intercept ---#
  logit_lf_0 <- glm(inlf~1,data = data,family=binomial(link='logit'))

  #--- extract LL using the logLik() function ---#
  LL0 <- logLik(logit_lf_0)
  LL0

  #--- extract LL using the logLik() function from your preferred model ---#
  LL <- logLik(logit_lf)
  LL

  #--- pseudo R2 ---#
  pR2 <- 1-LL/LL0
  pR2

@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]

  \begin{block_code}{R code: alternative (easier) calculation of pseudo-$R^2$}
<<r2_2, cache=TRUE, size='scriptsize'>>=
  #--- or more easily ---#
  1-logit_lf$deviance/logit_lf$null.deviance

  #--- what are deviances? ---#
  logit_lf$null.deviance # = -2*LL0
  logit_lf$deviance # = -2*LL
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Testing: joint significance}
    You can do Likelihood Ratio (LR) test:
    \begin{align*}
      LR = 2(LL_{unrestricted}-LL_{restricted}) \sim \chi^2_{df\_restrictions}
    \end{align*}
    where $df\_restrictions$ is the number of restrictions
  \end{block}
  \only<2->{\begin{block}{An example}
    \begin{itemize}
       \item $H_0:$ the coefficients on $exper$, $exper2$, and $age$ are $0$
       \item $H_1:$ $H_0$ is false
     \end{itemize}
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: LR test for joint significance}

<<lr_test, cache=TRUE, size='scriptsize'>>=
  #--- unrestricted ---#
  logit_ur <- glm(
    inlf~nwifeinc+educ+exper+exper2+age+kidslt6+kidsge6,
    data = data,family=binomial(link='logit')
    )

  #--- restricted ---#
  logit_r <- glm(
    inlf~nwifeinc+educ+kidslt6+kidsge6,
    data = data,family=binomial(link='logit')
    )

  #--- LR test using lrtest() from the lmtest package ---#
  library(lmtest)
  lrtest(logit_r,logit_ur)
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Prediction}
  After estimating a binary choice model, you can easily predict the following two
    \begin{itemize}
      \item $\hat{z}=\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k$
      \item $\widehat{Prob(y=1|\mathbf{x})}=G(\hat{z})=G(\hat{\beta}_0+\hat{\beta}_1 x_1+\dots+\hat{\beta}_k x_k)$
    \end{itemize}
  \end{block}
  \begin{block_code}{R code: LR test for joint significance}

<<pred, cache=TRUE, size='scriptsize'>>=
  #--- z hat ---#
  z <- predict(probit_lf,type='link')
  head(z)

  #--- G(z) hat ---#
  Gz <- predict(probit_lf,type='response')
  head(Gz)
@
  \end{block_code}
\end{frame}

\begin{frame}[c]
  \begin{block}{Marginal effect of an independent variable}
    \begin{itemize}
      \item Coefficient estimates across different models (probit and logit) are not meaningful because the same value of a coefficient estimate means different things
      \item They are the estimates of $\beta$s, not the direct impact of the independent variables on the $Prob(y=1)$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Calculating the marginal effect of an independent variable}
    \begin{align*}
    \frac{\partial Pr(y=1|x_1,\dots,x_k)}{\partial x_k} = G'(z)\times \beta_k
    \end{align*}
    \begin{itemize}
      \item the marginal impact depends on the current levels of all the independent variables
      \item we typically report one of the two types of marginal impacts
      \begin{itemize}
        \item the marginal impact \textcolor{blue}{at the mean} (average person): when all the independent variables take on their respective means
        \item the average of the marginal impacts calculated for each of all the individuals observed
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Marginal impact at the mean}
    \begin{align*}
    \frac{\partial Pr(y=1|\bar{x_1},\dots,\bar{x_k})}{\partial x_k} = G'(\beta_0+\beta_1 \bar{x_1}+\dots+\beta_k\bar{x_k})\times \beta_k
    \end{align*}
  \end{block}

  \begin{block}{Mean marginal impact}
    \begin{align*}
    \sum_{i=1}^n \frac{\partial Pr(y_i=1|\bar{x_{i,1}},\dots,\bar{x_{i,k}})}{\partial x_k} = \sum_{i=1}^n G'(z_i)\times \beta_k
    \end{align*}
  \end{block}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: marginal impact of $educ$ at the mean}

<<mi, cache=TRUE, size='scriptsize'>>=
  #--- get the coef ---#
  probit_lf$coef

  #--- get the mean ---#
  means <- summarize(data,mean(nwifeinc),mean(educ),mean(exper),mean(exper2),
    mean(age),mean(kidslt6),mean(kidsge6))

  #--- calculate z ---#
  z <- probit_lf$coef[1] + sum(probit_lf$coef[-1]*means)

  #--- marignal impact ---#
  dnorm(z)*probit_lf$coef['educ']
@
  \end{block_code}
  \only<2->{\begin{block}{Interpretation}
    If your eduction goes up by $1$ year, you are $5\%$ more likely to be in the labor force when you are an average person
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: mean marginal impact of $educ$}

<<mean_mi, cache=TRUE, size='scriptsize'>>=
  #--- get z for all the individuals ---#
  z <- predict(probit_lf,type='link')

  #--- get G'(z) ---#
  Gz_indiv <- dnorm(z)

  #--- mean marignal impact of eduction ---#
  mean(Gz_indiv)*probit_lf$coef['educ']
@
  \end{block_code}
\end{frame}

% \begin{frame}[c,fragile]
%   \frametitle{Deriving probit and logit models using utility maximization framework}
%   \begin{block}{Utility Function}
%   \vspace{-0.6cm}
%     \begin{align*}
%     U(y|x) =  \mu_0 + y*(\beta_0+\beta_1 x_1+\dots + \beta_k x_k+v)
%     \end{align*}
%   \end{block}
%   \begin{block}{Utility when $y=0$ and $y=1$}
%   \vspace{-0.6cm}
%   \begin{align*}
%     y=0:\;\; &U(y=0|x)=\mu_0 \\
%     y=1:\;\; &U(y=1|x)=\mu_0 + (\beta_0+\beta_1 x_1+\dots + \beta_k x_k+v)
%   \end{align*}
%   \vspace{-0.6cm}
%      \begin{itemize}
%       \item $\mu_0$ is the part of utility you get whether $y=0$ or $y=1$
%       \item $(\beta_0+\beta_1 x_1+\dots + \beta_k x_k+v)$ is the \textcolor{blue}{additional} utility you would gain if $y=1$ relative to when $y=0$
%       \item $v$ is the unobservable (to the econometricians) part of the additional utility
%     \end{itemize}
%   \end{block}
% \end{frame}
%
% \begin{frame}[c]
%   \begin{block}{Utility maximization}
%     One would choose $y=1$ if $U(y=0|x)<U(y=1|x)$, that is
%     \begin{align*}
%       \mu_0 + (\beta_0+\beta_1 x_1+\dots + \beta_k x_k+v) - \mu_0 >0\\
%       \beta_0+\beta_1 x_1+\dots + \beta_k x_k+v > 0
%     \end{align*}
%   \end{block}
%   \begin{block}{Probability of choosing $y=1$}
%   \vspace{-0.6cm}
%   \begin{align*}
%     Prob(y=1|\mathbf{x}) = Prob(-v<\beta_0+\beta_1 x_1+\dots + \beta_k x_k)
%   \end{align*}
%   \end{block}
%   \begin{block}{If the error term $-v|x\sim N(0,1)$}
%   \vspace{-0.6cm}
%   \begin{align*}
%     Prob(y=1|\mathbf{x}) = \Phi(\beta_0+\beta_1 x_1+\dots + \beta_k x_k),
%   \end{align*}
%   where $\Phi()$ is the cumulative distribution function of standard normal ditribution
%   \end{block}
% \end{frame}
%
%
% \begin{frame}[c]
%   \begin{block}{If the error term $-v|x\sim N(0,1)$}
%   \vspace{-0.4cm}
%   \begin{align*}
%     Prob(y=1|\mathbf{x}) = \Phi(\beta_0+\beta_1 x_1+\dots + \beta_k x_k),
%   \end{align*}
%   where $\Phi()$ is the cumulative distribution function of standard normal ditribution
%   \end{block}
%   \only<2->{\begin{block}{So,}
%      When you say $Prob(y=1|\mathbf{x}) = \Phi(\beta_0+\beta_1 x_1+\dots + \beta_k x_k)$, you are assuming that the additional utility of $y=1$ (owning at least one car) is represented by $\beta_0+\beta_1 x_1+\dots + \beta_k x_k+v$, where $\beta_0+\beta_1 x_1+\dots + \beta_k x_k$ is the observable (to you) part and the unobseravble part $v$ is distributed as $N(0,1)$ conditional on $x_1,\dots,x_k$.
%    \end{block}}
%    % \only<3->{\begin{block}{Note}
%    %   This framework helps when we learn multinomial choice modeling
%    % \end{block}}
% \end{frame}
%
% \begin{frame}[c]
% \begin{block}{Notes}
%   \begin{itemize}
%   \item If the error term $v$ is correlated with any of $x_1,\dots,x_k$, your coefficient estimation is biased (endogeneity)
%   \item If the error term $v$ is not Normally distributed, your estimation is biased except special cases (specification problem)
%   \item Remember, the unbiasednedd property of OLS on linear models is \textcolor{blue}{NOT} subject to mis-specification of the distribution of the error term because we don't even model the distribution of the error term in the first place
% \end{itemize}
% \end{block}
% \end{frame}
%
%
%
\begin{frame}
\title{Regression Models for  Count Data}
\author{}
\date{}
\titlepage
\end{frame}


\begin{frame}[c,fragile]
  \begin{block}{Count as the dependent variable}
  Count variables take \textcolor{blue}{non-negative discrete} values ($0,1,\dots,$)
    \begin{itemize}
      \item the number of times people get arrested in a year
      \item the number of cars owned by a family
      \item the number of patents applied for by a firm in a year
      \item the number of kids in a family
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Poisson regression}
    By far the most pupular choice to analyze count variables is \textcolor{blue}{Poisson regression}
    \begin{itemize}
      \item The outcome (count) variable is assumed to be Poisson distributed
      \item The mean of the Poisson distribution is assumed to be a function of some variables you believe matter
    \end{itemize}
  \end{block}
  \only<2->{
  \begin{block}{Poisson distribution}
    Poisson distribution is \textcolor{blue}{a} discrete probability distribution that describes the probability of the number of events that occur in a fixed interval of time and/or space
    \begin{align*}
      Prob(y|\lambda)=\frac{\lambda^y e^{-\lambda}}{y!}, \;\; \mbox{where} \;\; \lambda=E[y]
    \end{align*}
  \end{block}
  }

\end{frame}

\begin{frame}[c,fragile]
<<poisson, echo=FALSE, cache=TRUE, size='scriptsize',out.height='2.5in',out.width='2.5in'>>=
  x <- 0:15
  plot_data <- data.table(
    x=x,
    ' E[y]=1 ' = dpois(x,lambda=1),
    ' E[y]=3 ' = dpois(x,lambda=3),
    ' E[y]=5 ' = dpois(x,lambda=5)
  ) %>%
  melt(id.vars='x')

  ggplot(data=plot_data) +
    geom_bar(
      aes(x=factor(x),y=value,fill=variable),
      color='black',stat='identity',position='dodge') +
    ylab('probability') +
    scale_fill_discrete(name='') +
    xlab('the number of events') +
    theme(
      legend.position='bottom'
    )
@
  \begin{block}{Poisson regression}
    We try to learn what and how variables affect the expected value (the expected number of events conditional on independent variables)
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Expected number of events conditional on independent variables}
    \vspace{-0.4cm}
    \begin{align*}
    E[y|\mathbf{x}] = G(\beta_0+\beta_1 x_1+\dots + \beta_k x_k)
    \end{align*}
    \vspace{-0.6cm}
    \begin{itemize}
      \item This is exactly the same modeling framework we used
        \begin{itemize}
          \item Linear model: $G(z)=z$
          \item Probit model: $G(z)=\Phi(z)$
        \end{itemize}
    \end{itemize}
  \end{block}
  \only<2->{\begin{block}{A popular choice of $G()$}
  \vspace{-0.4cm}
  \begin{align*}
    G(z) = exp(z)
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
    \item ensures that the expected value conditional on $\mathbf{x}$ is always positive
  \end{itemize}
  \end{block}}
\end{frame}


\begin{frame}[c]
  \only<1-3>{\begin{block}{The number of events for two individuals}
  \begin{itemize}
  \item Individual 1: $y=3$ (own three cars)
  \item Individual 2: $y=1$ (own one car)
  \end{itemize}
  \end{block}}

  \only<2-3>{
  \begin{block}{Expected number of events observed}
  \vspace{-0.6cm}
    \begin{align*}
    \mbox{Individual 1}:& \lambda_1= exp(z_1) \\
    \mbox{Individual 2}:& \lambda_2= exp(z_2)
    \end{align*}
  \end{block}
  }

  \only<3-4>{
  \begin{block}{Probability of observing the number of events we observed}
  \vspace{-0.6cm}
    \begin{align*}
    \mbox{Individual 1}:& Prob(y=3|\mathbf{x_1})=\frac{\lambda_1^3 e^{-\lambda_1}}{3!}  \\
    \mbox{Individual 2}:& Prob(y=1|\mathbf{x_2})= \frac{\lambda_2^1 e^{-\lambda_2}}{1!}
    \end{align*}
  \end{block}
  }

  \only<4->{
  \begin{block}{Probability of observing a series of events by all individuals}
  The probability that we observe a \textcolor{blue}{collection of choices} made by them (if their events are independent)
    \begin{align*}
      L=Prob(y_1=3|\mathbf{x_1})\times Prob(y_2=1|\mathbf{x_2}) = \frac{\lambda_1^3 e^{-\lambda_1}}{3!}\times\frac{\lambda_2^1 e^{-\lambda_2}}{1!}
    \end{align*}
  which we call \textcolor{blue}{likelihood function}.
  \end{block}
  }

  \only<5->{
  \begin{block}{Log-likelihood function}
  \vspace{-0.6cm}
    \begin{align*}
      LL=log(L)&= log(\frac{\lambda_1^3 e^{-\lambda_1}}{3!}) + log(\frac{\lambda_2^1 e^{-\lambda_2}}{1!})
    \end{align*}
    \textcolor{blue}{(Remember $\lambda_i=exp(\beta_0+\beta_1 x_{i,1}+\dots + \beta_k x_{i,k})$)}
  \end{block}
  }

  \only<6->{
  \begin{block}{MLE}
  \vspace{-0.6cm}
    \begin{align*}
      Max_{\beta_1,\dots,\beta_k}\;\; & LL
    \end{align*}
  \end{block}
  }

\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Implementation in R with an example}
  The number of times a man is arrested during 1986:
  \begin{align*}
  Pr(narr86|\mathbf{x})= G(z)
  \end{align*}
  where
  \begin{align*}
  z = & \beta_0+\beta_1 pcnv+ \beta_2 tottime + \beta_3 qemp86 + \beta_4 inc86\\
    & + \beta_5 black + \beta_6 hispan
  \end{align*}
  \vspace{-0.6cm}
  \begin{itemize}
    \item $narr86$: \# of times arrested in 1986
    \item $pcnv$: proportion of prior conviction
    \item $tottime$: time in prison since 18
    \item $qemp86$: \# quarters employed in 1986
    \item $inc86$: legal income in 1986 (in $\$100$)
  \end{itemize}

  \end{block}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: importing the data}
<<data_pois, cache=TRUE, size='scriptsize'>>=
    #--- import the data ---#
    data <- read.dta13('CRIME1.dta')

    #--- take a look ---#
    dplyr::select(data,narr86,pcnv,qemp86,inc86)
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: Poisson model estimation using glm()}

<<pois, cache=TRUE, size='scriptsize'>>=
  pois_lf <- glm(

  #--- formula ---#
  narr86~pcnv+tottime+qemp86+inc86+black+hispan,

  #--- data ---#
  data = data,

  #--- models ---#
  family=poisson(link='log')
  )
@
  \end{block_code}
  \begin{block}{$family$ option}
    \begin{itemize}
      \item $poisson()$: tells $R$ that your dependent variable is Poisson distributed
      \item $link='log'$: tells $R$ that you want to use $exp()$ (the inverse of $log()$) as $G()$ in $E(y=1|\mathbf{x})=G(z)$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: summary}

<<pois_sum, cache=TRUE, size='scriptsize'>>=
  summary(pois_lf)$coef
@

  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
<<eval=FALSE, size='scriptsize'>>=
  stargazer(pois_lf,type='latex',no.space=TRUE,table.layout='-ldc-t-s-')
@

  \scriptsize{
<<stargaze_pois, echo=FALSE, cache=TRUE, size='scriptsize',results='asis'>>=
  stargazer(pois_lf,type='latex',no.space=TRUE,table.layout='-ldc-t-s-')
@
  }
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Calculating the marginal impact of an independent variable}
    \begin{align*}
    \frac{\partial E(y|x_1,\dots,x_k)}{\partial x_k} = \beta_k\times exp(\beta_0+\beta_1 x_1+\dots + \beta_k x_k)
    \end{align*}
    \begin{itemize}
      \item the marginal impact depends on the current levels of all the independent variables
      \item we typically report one of the two types of marginal impacts
      \begin{itemize}
        \item the marginal impact \textcolor{blue}{at the mean} (average person): when all the independent variables take on their respective means
        \item the average of the marginal impacts calculated for each of all the individuals observed
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: mean marginal impact of income}

<<mean_mi_pois, cache=TRUE, size='scriptsize'>>=
  #--- get z for all the individuals ---#
  z <- predict(pois_lf,type='link')

  #--- get G'(z) ---#
  Gz_indiv <- exp(z)

  #--- mean marignal impact of eduction ---#
  mean(Gz_indiv)*pois_lf$coef['inc86']
@
  \end{block_code}
  \only<2->{\begin{block}{Interpretation}
    If your income goes up by $\$100$, the expected number of getting arrested declines by $0.0034$
  \end{block}}
\end{frame}

\begin{frame}[c,fragile]
\begin{block}{Notes}
  \begin{itemize}
    \item Poisson regression model is under the same econometric modeling framework: GLM
    \item Codes for testing are exactly the same as those we saw for the binary response models
  \end{itemize}
\end{block}
\end{frame}


\begin{frame}
\title{Multinomial Choice}
\author{}
\date{}
\titlepage
\end{frame}

\begin{frame}[c,fragile]
  \begin{block}{Multinomial Choice}
    Instead of two options, you may be picking one option out of more than two options
    \begin{itemize}
      \item which carrier?
        \begin{itemize}
           \item Verizon
           \item Sprint
           \item AT\&T
           \item T-mobile
         \end{itemize}
      \item which transportation means to commute?
      \begin{itemize}
        \item own car
        \item Uber
        \item bus
        \item train
        \item bike
      \end{itemize}
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[t]
  \begin{block}{Multinomial logit model}
    The most popular model to analyze multinomial choice
    \begin{itemize}
      \item environmental evaluation
      \item tranposrtation
      \item marketing
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c]
\frametitle{Understanding multinomial logit model through an example}
  \begin{block}{Choice of trains}
    \begin{enumerate}
      \item 10 euros, 30 minutes travel time, one change
      \item 20 euros, 20 minutes travel time, one change
      \item 22 euros, 22 minutes travel time, no change
    \end{enumerate}
  \end{block}
  \begin{block}{Associated utility}
    \begin{enumerate}
      \item $V_1=\alpha_1 + \beta 10 + \gamma 30 + \rho 1 + v_1$
      \item $V_2=\alpha_2 + \beta 20 + \gamma 20 + \rho 1 + v_2$
      \item $V_3=\alpha_3 + \beta 22 + \gamma 22 + \rho 0 + v_3$
    \end{enumerate}
  \end{block}
\end{frame}


% %-------- start of frame --------%

% \begin{frame}[c,fragile]
%   \frametitle{Which option to pick?}
%   \begin{itemize}
%     \item An individual would pick the option that provides the highest utility
%   \end{itemize}
%   \begin{block}{Pick option 1 if}
%     $V_1 > V_2$ and $V_1 > V_3$ ($V_1=max{V_1,V_2,V_3}$)
%   \end{block}
% \end{frame}

% %-------- end of frame --------%

\begin{frame}[c]
  \begin{block}{Choice probability}
  Logit model assumes that the probability of choosing an alternative is the following:
    \begin{enumerate}
      \item $P_1=\frac{e^{V_1}}{e^{V_1}+e^{V_2}+e^{V_3}}$
      \item $P_2=\frac{e^{V_2}}{e^{V_1}+e^{V_2}+e^{V_3}}$
      \item $P_3=\frac{e^{V_3}}{e^{V_1}+e^{V_2}+e^{V_3}}$
    \end{enumerate}
  \end{block}
  \only<2->{\begin{block}{Notes}
    \begin{itemize}
      \item $0<P_j<1$, $^\forall j=1,2,3$
      \item $\sum_{j=1}^3=1$
    \end{itemize}
  \end{block}}
\end{frame}

\begin{frame}[c]
  \begin{block}{Probability of observing individual $i$ choosing what $i$ chose}
  \vspace{-0.4cm}
  \begin{align*}
    P_i = \Pi_{j=1}^3 y_{i,j}\times P_j
  \end{align*}
  where $y_{i,j}=1$ if $i$ chose $j$, 0 otherwise
  \end{block}
  \begin{block}{An example}
  \vspace{-0.4cm}
    \begin{align*}
      & y_{i,1} = 0, \;\;y_{i,2} = 1,\;\;y_{i,3} = 0\\
      & P_i = \Pi_{j=1}^3 y_{i,j}\times P_j = 0\times P_1 + 1\times P_2 + 0\times P_3
    \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c]
  \begin{block}{Probability of observing a series of chocies made by all the subjects}
  If choices made by the subjects are independent with each other,
  \begin{align*}
    LL = \Pi_{i=1}^n P_i = \Pi_{i=1}^n \Pi_{j=1}^3 y_{i,j}\times P_j
  \end{align*}
  \end{block}
  \begin{block}{MLE}
  \vspace{-0.6cm}
  \begin{align*}
    Max_{\beta,\gamma,\rho}\;\; log(LL)
  \end{align*}
  \end{block}
\end{frame}

\begin{frame}[c]
\frametitle{Interpreatation of the coefficients}
\begin{block}{Model in general}
  \begin{align*}
    V_{i,j} & = \alpha_j + \beta_1 x_{1,i,j} + \dots + \beta_k x_{k,i,j} \\
    P_{i,j} & = \frac{e^{V_{i,j}}}{\sum_{k=1}^J e^{V_{i,k}}}
  \end{align*}
  \end{block}

  \begin{block}{Interpreatation of the coefficients}
    \begin{align*}
      \frac{\partial P_{i,j}}{\partial x_{k,i,j}} & = \beta_k P_{i,j}(1-P_{i,j})
    \end{align*}
    \begin{itemize}
      \item \textcolor{blue}{A marginal change in $k$th variable for alternative $j$ would change the probability of choosing alternative $j$ by $\beta_k P_{i,j}(1-P_{i,j})$}
      \item the sign of the impact is the same as the sign of the coefficient
    \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}[c,fragile]
%   \begin{block}{Random Utility Framework}

%   Let $i$ and $j$ be indices for individual and option (alternative), respectively. Then, one may write individual $i$'s utility of option $j$ may be written as follows:
%   \begin{align*}
%     U_{i,j} = & \mbox{observable part} \;\; \\
%             & + \mbox{unobservable (to econometrician) part} \\
%             = & (\beta_0 + \beta_1 x_{i,j,1} + \dots + \beta_k x_{i,j,k}) \\
%             & + v_{i,j}
%   \end{align*}
%   \textcolor{blue}{(we used a similar framwork in the binary choice models)}
%   \end{block}

% \end{frame}

% \begin{frame}[c,fragile]
%   \begin{block}{Utility maximization}
%     Individual $i$ would pick $j$ if $j$ gives the maximum utility to $i$.
%   \end{block}
%   \begin{block}{Mathematically}
%     If $U_{i,j} > U_{i,k},\;\; ^\forall k \ne j$, $i$ choose $j$, which means
%     \begin{align*}
%     U_{i,j} - U_{i,k} > 0, \;\; ^\forall k \ne j\\
%     X_{i,j}\beta + v_{i,j} - (X_{i,k}\beta + v_{i,k})> 0, \;\; ^\forall k \ne j \\
%     v_{i,j} - v_{i,k}> X_{i,k}\beta - X_{i,j}\beta, \;\; ^\forall k \ne j
%     \end{align*}
%   \end{block}
%   \begin{block}{Probability of observing choice of $j$ made by $i$}
%   \vspace{-0.6cm}
%   \begin{align*}
%     Prob(y_i=j|\mathbf{x})=Prob(v_{i,j} - v_{i,k}> X_{i,k}\beta - X_{i,j}\beta, \;\; ^\forall k \ne j)
%   \end{align*}
%   \end{block}
% \end{frame}

% \begin{frame}[c,fragile]
%   \begin{block}{McFadden (1978)}
%     If $v_{i,j}$ is an independently distributed type II extreme value distribution,
%     \begin{align*}
%     Prob(y_i=j|\mathbf{x})= & Prob(v_{i,j} - v_{i,k}> X_{i,k}\beta - X_{i,j}\beta, \;\; ^\forall k \ne j) \\
%     = & \frac{exp(X_{i,j}\beta)}{1+\sum_{k=1}^J exp(X_{i,k}\beta)}
%     \end{align*}
%   \end{block}
% \end{frame}

% \begin{frame}[c]
%   \begin{block}{Example: choice of heating system in California}
%   900 observations on the choice from
%     \begin{itemize}
%       \item gas control (gc)
%       \item gas room (gr)
%       \item electric centrol (ec)
%       \item electric room (er)
%       \item heat pump (gc)
%     \end{itemize}
%   \end{block}
% \end{frame}

% \begin{frame}[c]
%   \begin{itemize}
%     \item $idcase$: observation number
%     \item $depvar$: chose alternative
%     \item $ic.alt$: installation cost
%     \item $oc.alt$: annual operating cost
%     \item $income$: annual income of the household
%     \item $agehead$: the age of the household head
%     \item $rooms$: the number of rooms in the house
%     \item $region$: northen coastal ($ncostl$), southern coastal ($scostl$), mountain ($mountn$), and central valley ($valley$)
%   \end{itemize}
% \end{frame}

\begin{frame}[c]
  \begin{block}{Implementation in $R$}
    You can use $mlogit$ package to estimate multinomial logit models
    \begin{itemize}
      \item format your data in a specific manner
      \item convert your data using $mlogit.data()$
      \item estimate using $mlogit()$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: multinomial logit model}

<<mlogit_prep, cache=TRUE, size='scriptsize'>>=
  #--- library ---#
  library(mlogit)

  #--- get the heating data from the mlogit package ---#
  data('TravelMode',package='AER')

  #--- take a look at the data ---#
  # first 10 rows
  head(TravelMode,10)
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: data preparation}

<<mlogit_convert, cache=TRUE, size='scriptsize'>>=
  #--- convert the data ---#
  TM <- mlogit.data(TravelMode,
    shape='long', # what format is the data in?
    choice='choice', # name of the variable that indicates choice made
    chid.var='individual', # name of the variable that indicates who made choices
    alt.var='mode' # the name of the variable that indicates options
    )

  #--- take a look at the data ---#
  # first 10 rows
  head(TM,10)
@
  \end{block_code}

\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: multinomial logit model estimation}

<<mlogit_est, cache=TRUE, size='tiny'>>=
  #--- estimate ---#
  ml_reg <- mlogit(choice~wait+vcost+travel,data=TM)

  #--- summary ---#
  summary(ml_reg)
@
  \end{block_code}
\end{frame}

\begin{frame}[c,fragile]
  \begin{block_code}{R code: understanding the results}
<<mlogit_coef, cache=TRUE, size='scriptsize'>>=
  #--- coefficient ---#
  summary(ml_reg)$coef
@
  \end{block_code}
  \begin{block}{Notes}
    \begin{itemize}
      \item intercept for $air$ is dropped ($air$ is the base)
      \begin{itemize}
        \item train:(intercept) is $-0.786$ means that train is less likely to be chosen if all the other \textcolor{blue}{included} variables are the same
      \end{itemize}
      \item the greater the travel time, the less likely the option is chosen
    \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}[c,fragile]
%   \begin{block_code}{R code: understanding the results}
%   <<mlogit_fit, cache=TRUE, size='scriptsize'>>=
%   #--- coefficient ---#
%   head(ml_reg$fitted,10)
%   @
%   \end{block_code}
% \end{frame}

\end{document}

